usage: eval_gpt.py [-h] [--test TEST [TEST ...]] [--model_fpath MODEL_FPATH]
                   [--pretrained_model_name PRETRAINED_MODEL_NAME]
                   [--gpu_id GPU_ID] [--batch_size BATCH_SIZE]
                   [--no_repeat_ngram_size NO_REPEAT_NGRAM_SIZE]
                   [--top_k TOP_K] [--top_p TOP_P] [--save_to SAVE_TO]
                   [--inp_max_len INP_MAX_LEN] [--tar_max_len TAR_MAX_LEN]
                   [-d]

optional arguments:
  -h, --help            show this help message and exit
  --test TEST [TEST ...]
                        Default=['data/book/test', 'data/paper/test']
  --model_fpath MODEL_FPATH
                        Default=None
  --pretrained_model_name PRETRAINED_MODEL_NAME
                        The pretrained model to use. Default=gogamza/kobart-
                        base-v1
  --gpu_id GPU_ID       Default=-1
  --batch_size BATCH_SIZE
                        Default=64
  --no_repeat_ngram_size NO_REPEAT_NGRAM_SIZE
                        Default=3
  --top_k TOP_K         Default=40
  --top_p TOP_P         Default=0.95
  --save_to SAVE_TO     Default=submission
  --inp_max_len INP_MAX_LEN
                        A value for slicing the input data. It is important to
                        note that the upper limit is determined by the
                        embedding value of the model you want to use.
                        Default=512
  --tar_max_len TAR_MAX_LEN
                        A value for slicing the output data. It is used for
                        model inference. if the value is too small, the
                        summary may be truncated before completion.
                        Default=160
  -d, --debug           Specifies the debugging mode. Default=False
